\documentclass{article}
\usepackage[margin=1.8in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{bibentry}
\nobibliography*

\setlength{\parindent}{1.0em}
\setlength{\parskip}{1.0em}
\newcommand{\bibnote}[2]{
  \noindent \bibentry{#1}
  \vspace{-1.0em}
  \begin{center}
    \begin{minipage}{4.5in}#2\end{minipage}
  \end{center}
  \vspace{2em}
}


\begin{document}

\bibnote{Bajcsy1988}{Actuating a camera intentionally is also known as active
vision. Active vision is by no means new, one of the earliest work can be
traced back to \cite{Bajcsy1988} where active vision is defined as a problem of
an intelligent data acquisition process.}

\bibnote{Montiel2006}{An issue with monocular slam has been the intialization,
since information from multiple images acquired during motion must be combined
to achieve accurate depth estimates. This has led algorithms to deviate from
desirable Gaussian uncertainty representation of the probabilistic filters. In
this paper, a inverse parameterization is introduced which permits efficient
and accurate representation of uncertainty during undelayed initalization and
beyond all within the standard EKF. The key concept is direct parameterization
of inverse depth, where there is a high degree of linearity.  Importantly, the
parameterization can cope with features which are far from the camera (little
parallax during motion).}

\bibnote{Achtelik2011}{Demonstrated that two MAVs equipped with onboard
monocular cameras and IMUs can form a flexible stereo rig. This was achieved by
performing feature correspondence in the overlapping field of view between the
MAVs to estimate the relative pose.}

\bibnote{Zou2013}{Multiple handheld cameras, but cameras are all synchronized
making it impractical for robotic applications, where the input of each camera
should be computed asynchronoously in order to cope with missing data and
delays. Additionally, it is assumed that all cameras observe the same scene at
the start.}

\bibnote{Richardson2013}{AprilCal guides users to collect a high-quality camera
images inorder to perform camera intrinsics calibration for a monocular
camera.}

\bibnote{Cunningham2013}{Presents a fully decentralized SLAM system where each
robot maintains a consistent augmented local map that combines local and
neighbourhood information, but the system has only been validated in
simulation.}

\bibnote{Zou2013}{CoSLAM obtains multiple monocular camera streams as input,
and groups cameras using a place recognition module. The disadvantage of this
system are, first, all cameras need to be synchronized.  Secondly, to
initialize the SLAM systems all cameras must observe the same scene. Lastly,
the processing of all data requires a graphics processing unit (GPU) to run at
real time.}

\bibnote{Morrison2016}{A collaborative SLAM system is the work of, where the
system is designed to work with multiple hand-held devices equipped with either
a monocular camera and IMU or stereo camera pair, along with a mobile phone or
autonomous robot for network and computing capabilities. Each agent performs
full SLAM on board, and a centralized server is used for storing and sharing
maps between agents.  However, this system exhibits few other collaborative
features, for one it does not perform a global optimization of the maps created
by different agents, nor are the map optimization work load shared amongst
other agents or server.}

\bibnote{Deutsch2016}{A system that can combine different pose graph based SLAM
systems running on the agents is presented. Additionally, the centralized
server informs agents about updates in their respective pose graphs. However,
the global map and sub-maps of other agents are not shared amongst the agents.}

\bibnote{Cieslewski2017}{Proposes a method that requires a similar amount of
data exchange between agents as a centralized approach without precluding any
matches. This is achieved by pre-assigning visual bag-of-words vocabulary to
different robots, and obtaining a candidate selection to choose which robot to
send the full query. Both works were validated in simulation only.}

\bibnote{Rebello2017}{Calibrates the extrinsics between a stereo camera pair,
where one camera is rigidly mounted and the other camera is actuated by a
gimbal. The key contribution of this work is the next-best-view selection and
using it to reduce the total amount of image data for a good extrinsic
calibration.}

\bibnote{Cieslewski2017}{A rapid exploration method for a MAV in an unknown
environment is devised. The core contribution of this work lies in maintaining
a constant velocity while exploring an unknown environment, thereby reducing
the amount of wasted energy hovering in place. During outdoor experiments,
however, they experienced frequent visual odometry failures due to lighting
conditions and sudden quadrotor attitude changes.}

\bibnote{Palazzolo2017}{Proposed an information-driven path planning algorithm
for a MAV through an unknown environment. The MAV is assumed to be carrying a
forward-facing camera to perform object mapping, and has a 3D bounding box
around the target object of interest as a prior. This method is validated in
simulation only.}

\bibnote{Schmuck2018}{Collaborative centralized multi-agent SLAM (CCM-SLAM) The
system was designed to fuse data streams from multiple monocular cameras on
board multiple MAVs. Each MAV runs a visual odometry system for autonomy, and
offloads the computationally intensive tasks back to the central server. All
information between the MAVs and the central server are shared, for example if
an agent visits an area of the environment previously visited by another agent,
previous experiences are retrieved from the server. Finally, the server detects
and removes redundant data without compromising the robustness of the
estimation.}

\bibnote{Teixeira2018}{Argued that a collaborative relative pose estimation is
required to enable MAV flights close to structures that may have insufficient
texture near or on the target. The proposed method uses a master and slave
architecture, where the slave MAV is positioned close to the target structure,
and the master MAV acting as a reference to the slave is positioned further
away. The master is retrofitted with a known constellation of LED markers in
order for the slave MAV to estimate the relative pose.}


% BIBLIOGRAPHY
\bibliographystyle{ieeetr}
\bibliography{index}
\end{document}
